# Dynamic Model Switching Guide

**Reading time**: 15 minutes | **Skill level**: Intermediate | **Version**: v1.4.0 | **Last updated**: 2026-01-22

---

Le script `claude-switch` supporte maintenant le changement dynamique de mod√®le pour GitHub Copilot via la variable d'environnement `COPILOT_MODEL` et pour Ollama via `OLLAMA_MODEL`.

## Mod√®les disponibles via copilot-api

‚ö†Ô∏è **Important**: Tous les mod√®les ne sont pas compatibles avec l'endpoint `/chat/completions` utilis√© par copilot-api. Voir section Compatibilit√© ci-dessous.

### ‚úÖ Claude Models (Test√©s et fonctionnels)
- `claude-sonnet-4.5` ‚≠ê (d√©faut, meilleur rapport qualit√©/vitesse)
- `claude-opus-4.5` (meilleure qualit√©, plus lent)
- `claude-haiku-4.5` (le plus rapide)
- `claude-sonnet-4`

### ‚úÖ GPT Models (Compatibles avec `/chat/completions`)
- ‚úÖ `gpt-4.1` (d√©faut, 0x premium, √©quilibr√©) ‚≠ê
- ‚úÖ `gpt-5` (raisonnement avanc√©, 1x premium)
- ‚úÖ `gpt-5-mini` (ultra rapide, 0x premium)
- ‚ö†Ô∏è `gpt-4o` (stable mais **d√©pr√©ci√© le 17 f√©vrier 2026**)

### ‚ùå GPT Codex Models (INCOMPATIBLES - requi√®rent endpoint `/responses`)
- ‚ùå `gpt-5.2-codex` (GA depuis 14 jan 2026, mais `/responses` uniquement)
- ‚ùå `gpt-5.1-codex` (Preview, `/responses` uniquement)
- ‚ùå `gpt-5.1-codex-mini` (Preview, `/responses` uniquement)
- ‚ùå `gpt-5-codex` (Preview, `/responses` uniquement)

### ‚úÖ Gemini Models
- `gemini-3-pro-preview` (performant)
- `gemini-3-flash-preview` (rapide)

### ‚ö†Ô∏è Autres
- ‚úÖ `grok-code-fast-1` (rapide, sp√©cialis√© code)
- ‚úÖ `raptor-mini` (l√©ger, rapide)

### ‚ùå Mod√®les d√©pr√©ci√©s (17 f√©vrier 2026)
- `claude-opus-4.1` ‚Üí Utiliser `claude-opus-4.5` √† la place
- `gemini-2.5-pro` ‚Üí Utiliser `gemini-3-pro-preview` √† la place

## ‚ö†Ô∏è Compatibilit√© des mod√®les - Limitation Architecturale Majeure

### Probl√®me: Endpoint `/responses` vs `/chat/completions`

**copilot-api (v0.7.0) ne supporte QUE l'endpoint `/chat/completions`**. Tous les mod√®les GPT Codex (sp√©cialis√©s code) n√©cessitent le nouvel endpoint `/responses` lanc√© par OpenAI en octobre 2025.

### Incompatibilit√© Confirm√©e - Tous les Mod√®les Codex

**TOUS les mod√®les de la famille Codex sont INCOMPATIBLES** :

| Mod√®le | Status OpenAI | Erreur copilot-api |
|--------|---------------|-------------------|
| `gpt-5.2-codex` | GA (14 jan 2026) | ‚ùå "not accessible via /chat/completions endpoint" |
| `gpt-5.1-codex` | Preview | ‚ùå M√™me erreur |
| `gpt-5.1-codex-mini` | Preview | ‚ùå M√™me erreur |
| `gpt-5-codex` | Preview | ‚ùå M√™me erreur |

**Cause technique** : Les mod√®les Codex utilisent un paradigme stateful avec `previous_response_id` pour le contexte, incompatible avec l'API Chat Completions classique.

### Solutions de Contournement

**Option 1 : Utiliser les mod√®les GPT compatibles** (recommand√©) :
- `gpt-4.1` (0x premium, inclus, √©quilibr√©)
- `gpt-5` (1x premium, raisonnement avanc√©)
- `gpt-5-mini` (0x premium, rapide)

**Option 2 : Utiliser Claude via Copilot** (100% compatible) :
- `claude-sonnet-4.5` (meilleur rapport qualit√©/vitesse)
- `claude-opus-4.5` (qualit√© maximale)

**Option 3 : Attendre une mise √† jour copilot-api** :
- Requiert r√©√©criture du routeur API (modification majeure)
- Aucune roadmap annonc√©e √† ce jour
- PR communautaire en discussion mais pas de timeline

**Option 4 : Utiliser les interfaces natives** :
- VS Code native (Codex disponible)
- GitHub.com Chat (Codex disponible)
- copilot-cli (Codex disponible)

## M√©thodes pour changer de mod√®le

### M√©thode 1: Variable d'environnement (One-shot)

```bash
# Utiliser Opus pour une session
COPILOT_MODEL=claude-opus-4.5 claude-switch copilot

# Utiliser GPT-4.1 (compatible, inclus)
COPILOT_MODEL=gpt-4.1 ccc

# Utiliser Haiku (ultra rapide)
COPILOT_MODEL=claude-haiku-4.5 ccc
```

### M√©thode 2: Aliases pr√©d√©finis (Recommand√©)

D√©j√† configur√©s dans votre `~/.zshrc`:

```bash
# Apr√®s `source ~/.bash_aliases`:

ccc-opus     # Claude Opus 4.5 (meilleure qualit√©)
ccc-sonnet   # Claude Sonnet 4.5 (d√©faut)
ccc-haiku    # Claude Haiku 4.5 (ultra rapide)
ccc-gpt      # GPT-4.1 (alternative GPT, 0x premium)
```

### M√©thode 3: Export permanent (Session shell)

```bash
# D√©finir pour toute la session shell
export COPILOT_MODEL=claude-opus-4.5

# Tous les appels √† `ccc` utiliseront Opus
ccc
ccc
ccc

# Reset au d√©faut
unset COPILOT_MODEL
```

## Comparaison des mod√®les Claude via Copilot

| Mod√®le | Qualit√© | Vitesse | Usage recommand√© |
|--------|---------|---------|------------------|
| **claude-opus-4.5** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üê¢ Lent | Code critique, architecture |
| **claude-sonnet-4.5** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° Rapide | D√©veloppement quotidien (d√©faut) |
| **claude-haiku-4.5** | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° Ultra rapide | Refactoring simple, questions rapides |

## Exemples d'usage

### Workflow hybride

```bash
# Morning: Exploration rapide avec Haiku
ccc-haiku
> Explore this codebase structure

# Afternoon: D√©veloppement avec Sonnet (√©quilibre)
ccc-sonnet
> Implement user authentication

# Code Review: Qualit√© maximale avec Opus
ccc-opus
> Review this PR for security issues and architecture
```

### Comparaison de mod√®les

```bash
# Tester la m√™me question avec diff√©rents mod√®les
COPILOT_MODEL=claude-sonnet-4.5 ccc
> Optimize this algorithm

COPILOT_MODEL=gpt-4.1 ccc
> Optimize this algorithm

COPILOT_MODEL=claude-opus-4.5 ccc
> Optimize this algorithm
```

## Mod√®les GPT via Copilot

### GPT-4.1 (Recommand√©, √©quilibr√©)

```bash
COPILOT_MODEL=gpt-4.1 ccc
```

**Avantages**:
- ‚úÖ 100% compatible avec copilot-api (endpoint `/chat/completions`)
- 0x premium (inclus dans l'abonnement)
- √âquilibr√© qualit√©/vitesse
- Bon pour usage g√©n√©ral

**Limitations**:
- ‚ö†Ô∏è Validation MCP stricte (peut √©chouer sur certains outils)
- Workaround: `DISABLE_NON_ESSENTIAL_MODEL_CALLS=1`

**Usage**: Alternative GPT √† Claude Sonnet pour le d√©veloppement quotidien

### GPT-5 (Raisonnement avanc√©)

```bash
COPILOT_MODEL=gpt-5 ccc
```

**Avantages**:
- Raisonnement avanc√©
- Meilleure qualit√© que GPT-4.1

**Limitations**:
- 1x premium (co√ªt suppl√©mentaire)

**Usage**: T√¢ches complexes n√©cessitant raisonnement pouss√©

### GPT-5-mini (Ultra rapide)

```bash
COPILOT_MODEL=gpt-5-mini ccc
```

**Avantages**:
- Tr√®s rapide
- 0x premium (inclus)
- Bon pour questions simples

**Usage**: Refactoring basique, questions rapides

## Mod√®les Ollama (Local - Updated January 2026)

### Mod√®les recommand√©s

| Model | Size | SWE-bench | Context | Use Case |
|-------|------|-----------|---------|----------|
| **devstral-small-2** (default) | 24B | 68% | 256K native | Best agentic coding |
| ibm/granite4:small-h | 32B (9B active) | ~62% | 1M | Long context, 70% less VRAM |
| qwen3-coder:30b | 30B | 85% | 256K | Highest accuracy (needs template work) |

### Devstral-small-2 (Recommand√©)

```bash
# Default
cco
# Or explicit
OLLAMA_MODEL=devstral-small-2 cco
# Or with 64K context Modelfile (recommended)
OLLAMA_MODEL=devstral-64k cco
```

**Avantages**:
- ‚úÖ Meilleur mod√®le agentic pour coding (68% SWE-bench)
- ‚úÖ Format tool-calling Mistral/OpenAI standard ‚Üí compatible Claude Code
- ‚úÖ Pas de probl√®me "stuck on Explore" (contrairement √† Qwen2.5)
- 24B param√®tres ‚Üí ~15GB VRAM

**Usage**: D√©veloppement quotidien offline, code propri√©taire

### IBM Granite4 (Long Context)

```bash
OLLAMA_MODEL=ibm/granite4:small-h cco
# Or alias
cco-granite
```

**Avantages**:
- ‚úÖ Architecture hybride Mamba ‚Üí 70% moins de VRAM pour long contexte
- ‚úÖ 1M tokens contexte natif
- ‚úÖ 9B param√®tres actifs seulement ‚Üí rapide

**Limitations**:
- SWE-bench ~62% (inf√©rieur √† Devstral)

**Usage**: Projets avec beaucoup de fichiers, contexte limit√© RAM

### Configuration contexte 64K (CRITIQUE)

‚ö†Ô∏è **IMPORTANT**: Claude Code envoie ~18K tokens de system prompt + tools. Le contexte par d√©faut (4K) cause:
- Hallucinations
- Comportement "stuck on Explore"
- R√©ponses lentes (2-6 minutes au lieu de 5-15 secondes)

**Solution recommand√©e (Modelfile persistant)**:

```bash
# 1. Cr√©er le Modelfile
mkdir -p ~/.ollama
cat > ~/.ollama/Modelfile.devstral-64k << 'EOF'
FROM devstral-small-2
PARAMETER num_ctx 65536
PARAMETER temperature 0.15
EOF

# 2. Cr√©er le mod√®le
ollama create devstral-64k -f ~/.ollama/Modelfile.devstral-64k

# 3. Utiliser
OLLAMA_MODEL=devstral-64k cco
```

**V√©rifier le contexte effectif**: `ollama ps` (pas `ollama show`)

### Sources

- [Ollama Context Documentation](https://docs.ollama.com/context-length)
- [Taletskiy blog](https://taletskiy.com/blogs/ollama-claude-code/)
- [r/LocalLLaMA benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1plbjqg/)
- [Devstral HuggingFace](https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512)

## Logs avec mod√®les

Le script log maintenant le mod√®le utilis√©:

```bash
$ cat ~/.claude/claude-switch.log

[2026-01-21 16:30:12] [INFO] Provider: GitHub Copilot (via copilot-api) - Model: claude-opus-4.5
[2026-01-21 16:30:12] [INFO] Session started: mode=copilot:claude-opus-4.5 pid=12345
[2026-01-21 16:32:45] [INFO] Session ended: mode=copilot:claude-opus-4.5 duration=2m33s exit=0

[2026-01-21 16:35:01] [INFO] Provider: GitHub Copilot (via copilot-api) - Model: gpt-4.1
[2026-01-21 16:35:01] [INFO] Session started: mode=copilot:gpt-4.1 pid=12567
```

### Analyser l'usage par mod√®le

```bash
# Sessions par mod√®le
grep "mode=copilot:" ~/.claude/claude-switch.log | cut -d':' -f4 | sort | uniq -c

# Exemple output:
# 12 claude-sonnet-4.5
#  5 claude-opus-4.5
#  3 gpt-4.1
#  2 claude-haiku-4.5
```

## Ajouter vos propres aliases

√âditez `~/.zshrc`:

```bash
# Copilot models
alias ccc-gemini='COPILOT_MODEL=gemini-3-pro-preview claude-switch copilot'
alias ccc-grok='COPILOT_MODEL=grok-code-fast-1 claude-switch copilot'
alias ccc-gpt4='COPILOT_MODEL=gpt-4o-2024-11-20 claude-switch copilot'

# Ollama models (already in install.sh)
alias cco-devstral='OLLAMA_MODEL=devstral-small-2 claude-switch ollama'
alias cco-granite='OLLAMA_MODEL=ibm/granite4:small-h claude-switch ollama'

# Custom 64K Modelfile variant
alias cco-64k='OLLAMA_MODEL=devstral-64k claude-switch ollama'

# Reload
source ~/.zshrc
```

## Status avec mod√®le actuel

```bash
$ ccs
=== Claude Code Provider Status ===

Anthropic API:  ‚úì Reachable
copilot-api:    ‚úì Running (:4141)
Ollama:         ‚úì Running (1 models)

=== Recent Sessions ===
[2026-01-21 16:30:12] [INFO] Session started: mode=copilot:claude-opus-4.5 pid=12345
[2026-01-21 16:32:45] [INFO] Session ended: mode=copilot:claude-opus-4.5 duration=2m33s exit=0
```

## Troubleshooting

### Mod√®le non reconnu

Si vous sp√©cifiez un mod√®le inexistant:

```bash
COPILOT_MODEL=invalid-model ccc
```

copilot-api retournera une erreur. V√©rifiez les mod√®les disponibles:

```bash
# Dans les logs de copilot-api start
copilot-api start

# Vous verrez:
# ‚Ñπ Available models:
# - claude-sonnet-4.5
# - claude-opus-4.5
# ...
```

### Mod√®le par d√©faut

Sans `COPILOT_MODEL`, le script utilise `claude-sonnet-4.5` (meilleur compromis).

Pour changer le d√©faut, √©ditez `~/bin/claude-switch` ligne 100:

```bash
local model="${COPILOT_MODEL:-claude-opus-4.5}"  # Nouveau d√©faut: Opus
```

## Recommandations strat√©giques

### D√©veloppement quotidien
```bash
ccc-sonnet  # ou juste `ccc`
```

### Code critique (PR, prod)
```bash
ccc-opus
```

### Questions rapides
```bash
ccc-haiku
```

### Comparaison/exp√©rimentation
```bash
# Tester plusieurs mod√®les
ccc-sonnet  # Baseline Claude
ccc-gpt     # Alternative GPT
ccc-opus    # Maximum quality
```

### Co√ªt zero via Copilot

Tous ces mod√®les sont **gratuits** avec votre abonnement Copilot Pro+, contrairement √† l'API Anthropic directe:

```bash
# Gratuit (Copilot)
ccc-opus   # Claude Opus via Copilot = $0
ccc-gpt    # GPT-4.1 via Copilot = $0

# Payant (Anthropic Direct)
ccd --model opus  # ~$15/million tokens input
```

## Conclusion

Vous avez maintenant acc√®s √† **15+ mod√®les compatibles** via une seule commande:

```bash
# Claude family
ccc-opus, ccc-sonnet, ccc-haiku

# GPT family
ccc-gpt (+ custom aliases)

# Autres
COPILOT_MODEL=<model> ccc
```

Le tout **gratuitement** avec votre abonnement Copilot Pro+ existant.

---

**Astuce Pro**: Cr√©ez un alias pour votre mod√®le pr√©f√©r√©:

```bash
alias cc='ccc-sonnet'  # Votre go-to command
```

---

## üìö Related Documentation

- [Command Reference](COMMANDS.md) - All available commands
- [Decision Trees](DECISION-TREES.md) - Which model for which task
- [Best Practices](BEST-PRACTICES.md) - Strategic model selection
- [MCP Profiles](MCP-PROFILES.md) - Model compatibility with MCP
- [FAQ](FAQ.md) - Model switching questions

---

**Back to**: [Documentation Index](README.md) | [Main README](../README.md)
