# Features Showcase - claude-switch

## ğŸ¯ Core Value Proposition

**Before claude-switch**:
```bash
# Stuck with one provider
export ANTHROPIC_API_KEY="<YOUR_API_KEY>"
claude
# ğŸ’¸ Costs $$ per session
# ğŸ”’ Can't go offline
# ğŸ­ Single model ecosystem
```

**After claude-switch**:
```bash
# Switch providers in 3 characters
ccd     # Anthropic Direct (production)
ccc     # GitHub Copilot (free prototyping)
cco     # Ollama Local (100% private)

# Switch models in one line
ccc-opus    # Claude Opus 4.5
ccc-gpt     # GPT-4.1
ccc-gemini  # Gemini Pro
```

---

## ğŸš€ Feature #1: Instant Provider Switching

### What It Does
Switch between 3 AI providers **without restarting** or **changing configs**.

### Live Example

```bash
# Morning: Prototype with free Copilot
ccc
â¯ Build a quick API endpoint
# âœ… Free usage, fast iteration

# Exit session (Ctrl+D)

# Afternoon: Production code review with Claude
ccd
â¯ Review security implications of the API
# âœ… Best quality analysis

# Exit session

# Evening: Offline work on sensitive code
cco
â¯ Refactor authentication module
# âœ… 100% local, no data leaves machine
```

**Impact**: 3 providers, 3 different use cases, **zero configuration changes**.

---

## ğŸ’° Feature #2: Dynamic Model Selection (25+ Models)

### What It Does
Access **15+ models from 3 providers** through a single interface.

### Model Matrix

| Provider | Free Models | Premium Models | Total |
|----------|-------------|----------------|-------|
| **Anthropic** | - | opus-4.5, sonnet-4.5, haiku-4.5 | 3 |
| **GitHub Copilot** | claude-*, gpt-4.1, gpt-5-mini | gpt-5, gemini-* | 12+ |
| **Ollama Local** | qwen2.5-coder, deepseek-coder, codellama | - | 10+ |

### Live Example: Switch Mid-Session

```bash
# Start with Claude Sonnet (default)
ccc
â¯ Analyze this complex algorithm
# Good but need deeper reasoning

# Exit and switch to Opus for deeper analysis
ccc-opus
â¯ Explain the time complexity proof
# âœ… Opus-level reasoning

# Switch to GPT-4.1 for comparison
ccc-gpt
â¯ Same question
# âœ… Different perspective
```

**Impact**: **3 models, 3 perspectives** on the same problem, no API juggling.

---

## ğŸ”’ Feature #3: MCP Profiles System

### The Problem
GPT-4.1 has **strict JSON schema validation** â†’ Many MCP servers fail.

**Before claude-switch**:
```
â¯ ccc-gpt
API Error: Invalid schema for 'mcp__grepai__grepai_index_status'
# ğŸ’¥ Session broken, no MCP tools available
```

**After claude-switch (v1.2.0)**:
```
â¯ ccc-gpt
Using restricted MCP profile for gpt-4.1
Injecting model identity prompt for gpt-4.1

â¯ /mcp
9 servers available (grepai excluded automatically)
# âœ… Works perfectly, no manual config
```

### How It Works

```
~/.claude/mcp-profiles/
â”œâ”€â”€ excludes.yaml              # Define problematic servers
â”œâ”€â”€ generate.sh                # Auto-generate profiles
â””â”€â”€ generated/
    â”œâ”€â”€ gpt.json              # GPT-compatible profile
    â””â”€â”€ gemini.json           # Gemini-compatible profile
```

**Impact**: **Zero errors**, automatic MCP compatibility for **all models**.

---

## ğŸ­ Feature #4: Model Identity Injection

### The Problem
GPT-4.1 running through Claude Code CLI **thinks it's Claude**.

**Before**:
```
â¯ ccc-gpt
â¯ qui es-tu ?

âº Je suis Claude, dÃ©veloppÃ© par Anthropic...
# âŒ Wrong! You're GPT-4.1!
```

**After (v1.2.0)**:
```
â¯ ccc-gpt
Injecting model identity prompt for gpt-4.1

â¯ qui es-tu ?

âº Je suis GPT-4.1, dÃ©veloppÃ© par OpenAI.
   ModÃ¨le : GPT-4.1 (OpenAI)
   Interface : Claude Code CLI (Anthropic)
# âœ… Correct identity
```

### How It Works

```bash
~/.claude/mcp-profiles/prompts/
â”œâ”€â”€ gpt-4.1.txt           # "You are GPT-4.1 by OpenAI..."
â””â”€â”€ gemini.txt            # "You are Gemini by Google..."
```

**Impact**: Models know **who they are**, users get **honest answers**.

---

## ğŸ¥ Feature #5: Health Checks & Fail-Fast

### What It Does
**Verify provider availability** before launching sessions.

### Live Example

```bash
# Copilot API not running
â¯ ccc
ERROR: copilot-api not running on :4141
  Start it with: copilot-api start
# âœ… Clear error, actionable fix

# After starting copilot-api
â¯ ccc
copilot-api health: OK
Provider: GitHub Copilot (via copilot-api) - Model: claude-sonnet-4.5
â”â”â” Claude Code [GitHub Copilot: claude-sonnet-4.5] â”â”â”
# âœ… Session starts
```

**Impact**: **No silent failures**, always know what's broken.

---

## ğŸ“Š Feature #6: Session Logging

### What It Does
Track **every session**: provider, model, duration, exit code.

### Live Example

```bash
â¯ tail ~/.claude/claude-switch.log

[2026-01-22 09:42:33] [INFO] Provider: GitHub Copilot - Model: gpt-4.1
[2026-01-22 09:42:33] [INFO] Using restricted MCP profile for gpt-4.1
[2026-01-22 09:42:33] [INFO] Injecting model identity prompt for gpt-4.1
[2026-01-22 09:42:33] [INFO] Session started: mode=copilot:gpt-4.1 pid=54464
[2026-01-22 10:15:20] [INFO] Session ended: duration=32m47s exit=0
```

**Impact**: **Full audit trail** of AI usage, troubleshooting, cost tracking.

---

## âš¡ Feature #7: Apple Silicon Optimization

### What It Does
Optimize **Ollama for M1/M2/M3/M4 chips** with Metal acceleration.

### Results (M4 Pro 48GB)

| Model | Before | After | Speedup |
|-------|--------|-------|---------|
| qwen2.5-coder:32b | 12 tok/s | **47 tok/s** | **3.9x** |
| deepseek-coder:33b | 10 tok/s | **43 tok/s** | **4.3x** |

### How To

```bash
# Run optimizer
./ollama-optimize.sh

# Verify
./ollama-check.sh
âœ… Metal GPU acceleration: Enabled
âœ… Model loaded in Metal VRAM: 19.2 GB
âœ… Performance: 47.3 tokens/sec
```

**Impact**: **Desktop-class LLM performance** on MacBook.

---

## ğŸ¬ Real-World Workflows

### Workflow 1: Cost-Optimized Development

```bash
# 1. Prototype features (free)
ccc
â¯ Build user authentication flow

# 2. Code review (paid, high quality)
ccd
â¯ Review security of auth implementation

# 3. Refactor sensitive parts (local, private)
cco
â¯ Optimize password hashing module
```

**Savings**: ~70% cost reduction vs Anthropic-only

---

### Workflow 2: Multi-Model Validation

```bash
# Test algorithm design with 3 different models
ccc-opus      # Claude Opus perspective
ccc-gpt       # GPT-4.1 perspective
ccc-gemini    # Gemini perspective

# Compare approaches, choose best
```

**Benefit**: **Cross-model validation** catches blind spots

---

### Workflow 3: Offline/Secure Development

```bash
# Work on proprietary code (airplane mode)
cco
â¯ Implement proprietary encryption algorithm
# âœ… No internet required
# âœ… Code never leaves machine
# âœ… Full Claude Code features
```

**Benefit**: **Zero data exfiltration risk**

---

## ğŸ“ˆ Comparison Table

| Feature | Without claude-switch | With claude-switch |
|---------|----------------------|-------------------|
| **Provider switching** | Manual env vars + restart | `ccc` / `ccd` / `cco` (3 chars) |
| **Model switching** | Change API config + restart | `COPILOT_MODEL=gpt-4.1 ccc` |
| **MCP compatibility** | Manual trial-and-error | Auto-detected profiles |
| **Model identity** | Confused (GPT thinks it's Claude) | Correct identity injection |
| **Health checks** | Silent failures | Fail-fast with actionable errors |
| **Session logs** | None | Full audit trail |
| **Cost optimization** | Single provider = $$ | Mix free/paid strategically |
| **Privacy** | Cloud-only | Offline mode available |
| **Apple Silicon perf** | Default (slow) | Metal-optimized (4x faster) |

---

## ğŸ¯ Key Metrics

- **3 providers** â†’ 1 unified interface
- **25+ models** â†’ Single command access
- **Zero config** â†’ Works with existing Claude Code
- **4x faster** â†’ Apple Silicon optimization
- **70% savings** â†’ Strategic provider mixing
- **100% private** â†’ Ollama offline mode
- **9/10 MCP** â†’ Auto-compatibility for strict models

---

## ğŸ’¡ Who Should Use This?

âœ… **Developers** who want cost control (mix free/paid)
âœ… **Teams** who need offline capability (air-gapped environments)
âœ… **Security-conscious** users (keep proprietary code local)
âœ… **Multi-model users** (validate with Claude + GPT + Gemini)
âœ… **Apple users** (M1/M2/M3/M4 optimization)
âœ… **Power users** (MCP servers, custom workflows)

---

## ğŸš€ Quick Start

```bash
# Install
curl -sSL https://raw.githubusercontent.com/YOU/claude-switch/main/install.sh | bash

# Use
ccc        # GitHub Copilot (free)
ccd        # Anthropic Direct (paid)
cco        # Ollama Local (private)
ccs        # Check status

# Advanced
ccc-gpt    # GPT-4.1 via Copilot
ccc-opus   # Claude Opus 4.5
OLLAMA_MODEL=deepseek-coder:33b cco  # Specific local model
```

**That's it.** Start switching. ğŸ¯
