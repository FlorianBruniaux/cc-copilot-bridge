# Troubleshooting Guide

**Reading time**: 20 minutes | **Skill level**: All levels | **Version**: v1.4.0 | **Last updated**: 2026-01-22

---

## üêå Ollama Extremely Slow or Hallucinating with Claude Code

### Symptom

```bash
cco
‚ùØ 1+1 ?
‚è±Ô∏è Response: 2-6 MINUTES (should be 3-10 seconds)
üíª CPU at 100%
üî• Mac fans spinning
# OR: hallucinations, "stuck on Explore" behavior, incoherent responses
```

### Root Cause

**Context mismatch**: Claude Code sends ~18K tokens of system prompt + tools, but default Ollama context is 4K tokens.

**Result**: Context is truncated ‚Üí model hallucinates ‚Üí regenerates constantly ‚Üí extreme slowness.

### Diagnosis

1. **Verify effective context**:
   ```bash
   # During cco session (in another terminal)
   ollama ps
   # Look at CONTEXT column - should be 65536 for Claude Code
   ```

2. **Check context usage in Claude Code**:
   ```bash
   # During session
   /context
   ```
   Look for: Free space negative or very low

### Solutions

#### Option 1: Create a 64K Modelfile (Recommended - Persistent)

This is the **official recommended method** from Ollama documentation. It persists across restarts.

```bash
# 1. Create Modelfile directory
mkdir -p ~/.ollama

# 2. Create the Modelfile
cat > ~/.ollama/Modelfile.devstral-64k << 'EOF'
FROM devstral-small-2
PARAMETER num_ctx 65536
PARAMETER temperature 0.15
EOF

# 3. Create the model variant
ollama create devstral-64k -f ~/.ollama/Modelfile.devstral-64k

# 4. Use the 64K model
OLLAMA_MODEL=devstral-64k cco
```

**Why this works**:
- `PARAMETER num_ctx` is embedded in the model ‚Üí persists across restarts
- `temperature 0.15` reduces hallucinations for code generation
- Custom model variants can be listed with `ollama list`

#### Option 2: Global Environment Variable (Quick Fix)

Less priority than Modelfile, but works as fallback:

```bash
# Set global context length
launchctl setenv OLLAMA_CONTEXT_LENGTH 65536
brew services restart ollama

# Verify
launchctl getenv OLLAMA_CONTEXT_LENGTH
# Should show: 65536
```

**Note**: Environment variable has lower priority than Modelfile PARAMETER.

#### Option 3: Use Copilot or Anthropic (Alternative)

**For large projects or when local performance is insufficient**:

```bash
# Copilot (free with subscription)
ccc
‚è±Ô∏è Response: 1-3 seconds ‚úÖ

# Anthropic Direct (paid)
ccd
‚è±Ô∏è Response: 1-2 seconds ‚úÖ
```

**Why this works**: Both handle 200K+ tokens context natively.

### Verify the Fix

```bash
# 1. Pull recommended model
ollama pull devstral-small-2

# 2. Create 64K Modelfile (see Option 1 above)

# 3. Start session
OLLAMA_MODEL=devstral-64k cco

# 4. Check effective context (in another terminal)
ollama ps
# Expected: CONTEXT = 65536

# 5. Test agentic task
‚ùØ create a file hello.py with print("Hello")
# Should complete in 5-15 seconds, not 2-6 minutes
```

### Memory Considerations (M4 Pro 48GB)

| Context Size | Model RAM | Cache RAM | Total | Free RAM |
|--------------|-----------|-----------|-------|----------|
| 32K | 15 GB | 4-6 GB | ~21 GB | ~27 GB |
| 64K | 15 GB | 8-12 GB | ~27 GB | ~21 GB |

**Recommendation**: Use 64K if possible. If RAM is tight, use 32K or switch to Copilot.

### Recommended Ollama Models (Updated January 2026)

| Model | Size | SWE-bench | Use Case |
|-------|------|-----------|----------|
| **devstral-small-2** (default) | 24B | 68% | Best agentic coding |
| ibm/granite4:small-h | 32B (9B active) | ~62% | Long context, 70% less VRAM |
| qwen3-coder:30b | 30B | 85% | Highest accuracy (needs template work) |

**Sources**:
- [Ollama Context Documentation](https://docs.ollama.com/context-length)
- [Taletskiy blog](https://taletskiy.com/blogs/ollama-claude-code/)
- [r/LocalLLaMA benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1plbjqg/)

---

## ‚ùå Model Not Found Error

### Symptom

```bash
cco
ERROR: Model devstral-small-2 not found
  Pull it with: ollama pull devstral-small-2
```

### Cause

Model not installed or model name mismatch.

### Solution

1. **Pull the recommended model**:
   ```bash
   ollama pull devstral-small-2
   ```

2. **Or pull backup model for long context**:
   ```bash
   ollama pull ibm/granite4:small-h
   ```

3. **Check installed models**:
   ```bash
   ollama list
   ```

4. **Override model if needed**:
   ```bash
   OLLAMA_MODEL=ibm/granite4:small-h cco
   ```

---

## üîë API Key Prompt on Every Launch

### Symptom

```bash
cco
Detected a custom API key in your environment
ANTHROPIC_API_KEY: <YOUR_API_KEY>
Do you want to use this API key?
  1. Yes
  2. No (recommended) ‚úì
```

### Cause

`ANTHROPIC_API_KEY` set in script triggers Claude Code validation.

### Solution

**Already fixed in v1.1.0+**. If using older version:

1. Edit `~/bin/claude-switch`
2. In `_run_ollama()` function, remove:
   ```bash
   export ANTHROPIC_API_KEY="ollama"
   ```
3. Keep only:
   ```bash
   export ANTHROPIC_BASE_URL="http://localhost:11434"
   export ANTHROPIC_AUTH_TOKEN="ollama"
   ```

---

## ‚ùå Model Not Accessible Error (copilot-api)

### Symptom

```bash
ccc-gpt  # or COPILOT_MODEL=gpt-5.2-codex ccc
ERROR  Failed to create chat completions Response { status: 400
ERROR  HTTP error: { error:
   { message: 'model gpt-5.2-codex is not accessible via the /chat/completions endpoint',
     code: 'unsupported_api_for_model' } }
```

### Cause

**TOUS les mod√®les de la famille GPT Codex** n√©cessitent l'endpoint OpenAI `/responses` (lanc√© en octobre 2025) au lieu du standard `/chat/completions`. copilot-api (v0.7.0) ne supporte que `/chat/completions`, rendant **TOUS les mod√®les Codex incompatibles** :

- ‚ùå `gpt-5.2-codex` (GA depuis 14 jan 2026)
- ‚ùå `gpt-5.1-codex` (Preview)
- ‚ùå `gpt-5.1-codex-mini` (Preview)
- ‚ùå `gpt-5-codex` (Preview)

**Cause technique** : Les mod√®les Codex utilisent un paradigme stateful avec `previous_response_id` pour le contexte, incompatible avec l'API Chat Completions classique.

### Solution

**Option 1: Utiliser des mod√®les GPT compatibles (Recommand√©)**

```bash
# Mod√®les GPT 100% compatibles avec copilot-api:
COPILOT_MODEL=gpt-4.1 ccc       # √âquilibr√©, 0x premium (inclus)
COPILOT_MODEL=gpt-5 ccc         # Raisonnement avanc√©, 1x premium
COPILOT_MODEL=gpt-5-mini ccc    # Ultra rapide, 0x premium
```

**Option 2: Utiliser Claude via Copilot (100% compatible)**

```bash
ccc-sonnet  # Claude Sonnet 4.5 (d√©faut, fiable)
ccc-opus    # Claude Opus 4.5 (meilleure qualit√©)
ccc-haiku   # Claude Haiku 4.5 (ultra rapide)
```

**Option 3: Suivre le d√©veloppement du fix**

Le PR communautaire [ericc-ch/copilot-api#117](https://github.com/ericc-ch/copilot-api/pull/117) travaille sur le support de l'endpoint `/responses`.

### Mod√®les test√©s et fonctionnels

| Mod√®le | Statut | Usage |
|--------|--------|-------|
| `claude-sonnet-4.5` | ‚úÖ Fonctionne | D√©veloppement quotidien |
| `claude-opus-4.5` | ‚úÖ Fonctionne | Code critique |
| `claude-haiku-4.5` | ‚úÖ Fonctionne | Questions rapides |
| `gpt-4.1` | ‚úÖ Fonctionne | Usage g√©n√©ral, 0x premium |
| `gpt-5` | ‚úÖ Fonctionne | Raisonnement avanc√©, 1x premium |
| `gpt-5-mini` | ‚úÖ Fonctionne | Ultra rapide, 0x premium |
| `gemini-3-pro-preview` | ‚úÖ Fonctionne | Alternative |
| `grok-code-fast-1` | ‚úÖ Fonctionne | Code sp√©cialis√© |
| `raptor-mini` | ‚úÖ Fonctionne | L√©ger |
| `gpt-5.2-codex` | ‚ùå Incompatible | Endpoint `/responses` requis |
| `gpt-5.1-codex` | ‚ùå Incompatible | Endpoint `/responses` requis |
| `gpt-5.1-codex-mini` | ‚ùå Incompatible | Endpoint `/responses` requis |
| `gpt-5-codex` | ‚ùå Incompatible | Endpoint `/responses` requis |

### Mod√®les d√©pr√©ci√©s (17 f√©vrier 2026)

```bash
# √Ä remplacer avant le 17 f√©vrier 2026:
claude-opus-4.1 ‚Üí claude-opus-4.5
gemini-2.5-pro ‚Üí gemini-3-pro-preview
```

---

## ‚ùå Reserved Billing Header Error (copilot-api)

### Symptom

```bash
ccc
‚ùØ 1+1
API Error: 400 Bad Request
{"error":{"message":"x-anthropic-billing-header is a reserved keyword and may not be used in the system prompt",
  "code":"invalid_request_body"}}
```

### Cause

**Claude Code v2.1.15+** injecte la cha√Æne `x-anthropic-billing-header` dans son system prompt pour le tracking billing interne. L'API Anthropic (via copilot-api proxy) rejette cette requ√™te car c'est un **mot-cl√© r√©serv√©** qui ne peut pas appara√Ætre dans les prompts utilisateur.

**Cause technique** : Anthropic r√©serve certains mots-cl√©s (comme `x-anthropic-billing-header`) pour son infrastructure interne. Quand Claude Code inclut ces mots dans le system prompt, l'API les d√©tecte et rejette la requ√™te pour √©viter les conflits.

**Issue GitHub** : [ericc-ch/copilot-api#174](https://github.com/ericc-ch/copilot-api/issues/174) (ouverte le 22 janvier 2026)

### Qui est affect√©

- ‚úÖ **Anthropic Direct (`ccd`)** : Non affect√© (API native g√®re correctement)
- ‚ùå **Copilot via copilot-api (`ccc`)** : AFFECT√â (proxy rejette le header)
- ‚úÖ **Ollama Local (`cco`)** : Non affect√© (pas d'API Anthropic)

### Solutions

**Option 1: Utiliser Anthropic Direct (Recommand√©)** ‚≠ê

```bash
ccd  # Anthropic API native, g√®re x-anthropic-billing-header correctement
```

**Avantages**:
- ‚úÖ 100% compatible avec toutes les versions Claude Code
- ‚úÖ Meilleure qualit√© (pas de proxy)
- ‚úÖ Support officiel Anthropic

**Inconv√©nients**:
- üí∞ Payant (facturation au token)

**Option 2: Utiliser Ollama Local**

```bash
cco  # 100% priv√©, pas d'API Anthropic
```

**Avantages**:
- ‚úÖ Gratuit, illimit√©
- ‚úÖ 100% priv√© (aucune donn√©e ne quitte la machine)
- ‚úÖ Pas affect√© par les probl√®mes API Anthropic

**Inconv√©nients**:
- üêå Plus lent que cloud (voir [Optimisation M4 Pro](OPTIMISATION-M4-PRO.md))

**Option 3: Attendre un fix de copilot-api**

L'issue est activement suivie sur GitHub. Possibles solutions en d√©veloppement :
1. Filtrage automatique du header r√©serv√© par copilot-api
2. Patch Claude Code pour exclure le header des proxies
3. Configuration Anthropic API pour accepter le header via proxies

**Suivi** : [ericc-ch/copilot-api#174](https://github.com/ericc-ch/copilot-api/issues/174)

### Workaround temporaire (NON RECOMMAND√â)

Un utilisateur a report√© que retirer manuellement `x-anthropic-billing-header` du system message permet de contourner l'erreur, mais :

‚ùå **Ne PAS utiliser** : Modifier le system prompt casse la session Claude Code
‚ùå **Fragile** : Cassera √† chaque update de Claude Code
‚ùå **Complexe** : N√©cessite d'intercepter et modifier les requ√™tes

**Pr√©f√©rez les Options 1 ou 2 ci-dessus.**

### Diagnostic

Si tu vois cette erreur sporadiquement :

```bash
# V√©rifier la version Claude Code
claude --version
# Si v2.1.15+, le probl√®me est pr√©sent

# V√©rifier les logs r√©cents
tail -50 ~/.claude/claude-switch.log | grep "400\|billing"

# Tester avec Anthropic Direct
ccd
‚ùØ 1+1
# Si √ßa fonctionne ‚Üí confirme que le probl√®me vient de copilot-api
```

### Patch communautaire (Solution avanc√©e)

**‚ö†Ô∏è AVERTISSEMENT** : Cette solution modifie le code source de copilot-api. √Ä utiliser uniquement si tu es √† l'aise avec le debugging et pr√™t √† restaurer en cas de probl√®me.

Un utilisateur de la communaut√© [@mrhanhan](https://github.com/ericc-ch/copilot-api/issues/174) a propos√© un patch fonctionnel qui filtre automatiquement le header r√©serv√©.

#### √âtape 1: Localiser le fichier √† patcher

```bash
# Trouver l'installation de copilot-api
which copilot-api
# ‚Üí /Users/YOU/.nvm/versions/node/vXX.XX.X/bin/copilot-api

# Le fichier √† modifier est dans dist/main.js
# Exemple: ~/.nvm/versions/node/v22.18.0/lib/node_modules/copilot-api/dist/main.js
```

#### √âtape 2: Cr√©er un backup

```bash
cd ~/.nvm/versions/node/v22.18.0/lib/node_modules/copilot-api/dist
cp main.js main.js.backup
echo "‚úÖ Backup cr√©√©: main.js.backup"
```

#### √âtape 3: Appliquer le patch

√âditer `dist/main.js` et trouver la fonction `translateAnthropicMessagesToOpenAI` (autour de la ligne 897).

**Avant (original)** :
```javascript
function translateAnthropicMessagesToOpenAI(anthropicMessages, system) {
	const systemMessages = handleSystemPrompt(system);
	const otherMessages = anthropicMessages.flatMap((message) =>
		message.role === "user" ? handleUserMessage(message) : handleAssistantMessage(message)
	);
	return [...systemMessages, ...otherMessages];
}
```

**Apr√®s (patch√©)** :
```javascript
function translateAnthropicMessagesToOpenAI(anthropicMessages, system) {
	let systemMessages = handleSystemPrompt(system);
	// FIX #174: Filter x-anthropic-billing-header from system prompt
	systemMessages = systemMessages.map((it) => {
		if (typeof it.content === "string" && it.content.startsWith("x-anthropic-billing-header")) {
			it.content = it.content.replace(
				/x-anthropic-billing-header: \?cc_version=.+; \?cc_entrypoint=\\+\n{0,2}\./,
				""
			);
			console.info('Filtered x-anthropic-billing-header from system message');
		}
		return it;
	});
	const otherMessages = anthropicMessages.flatMap((message) =>
		message.role === "user" ? handleUserMessage(message) : handleAssistantMessage(message)
	);
	return [...systemMessages, ...otherMessages];
}
```

**Modifications apport√©es** :
1. `const systemMessages` ‚Üí `let systemMessages` (ligne 2)
2. Ajout du filtre `systemMessages.map()` (lignes 3-12)
3. Log de confirmation quand le header est filtr√©

#### √âtape 4: Red√©marrer copilot-api

```bash
# Arr√™ter le processus actuel
kill $(ps aux | grep "copilot-api start" | grep -v grep | awk '{print $2}')

# Red√©marrer avec le patch
copilot-api start
```

#### √âtape 5: Tester le patch

**Test automatique** :

Un script de test est disponible dans le projet cc-copilot-bridge :

```bash
# Dans le d√©p√¥t cc-copilot-bridge
./scripts/test-billing-header-fix.sh
```

**Test manuel** :

```bash
# Test 1: Requ√™te avec billing header
curl -s -X POST http://localhost:4141/v1/messages \
  -H "Content-Type: application/json" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-sonnet-4.5",
    "max_tokens": 100,
    "system": "x-anthropic-billing-header: test\n\nYou are helpful.",
    "messages": [{"role": "user", "content": "Say hello"}]
  }' | jq '.content[0].text // .error'

# R√©sultat attendu: R√©ponse normale (pas d'erreur 400)
```

**Test avec Claude Code** :

```bash
ccc
‚ùØ 1+1
```

**R√©sultat attendu** : R√©ponse normale sans erreur `invalid_request_body`

#### V√©rification des logs

Dans le terminal o√π copilot-api tourne, tu devrais voir :

```
Filtered x-anthropic-billing-header from system message
```

Chaque fois que Claude Code envoie une requ√™te avec le header r√©serv√©.

#### Restaurer l'original

Si le patch cause des probl√®mes :

```bash
# Arr√™ter copilot-api
kill $(ps aux | grep "copilot-api start" | grep -v grep | awk '{print $2}')

# Restaurer le backup
cd ~/.nvm/versions/node/v22.18.0/lib/node_modules/copilot-api/dist
cp main.js.backup main.js

# Red√©marrer
copilot-api start
```

#### Limitations du patch

**‚ö†Ô∏è Patch temporaire** :
- ‚ùå Sera √©cras√© √† chaque `npm update copilot-api`
- ‚ùå Non test√© sur toutes les versions de copilot-api
- ‚ùå Peut ne pas couvrir tous les cas edge

**Apr√®s update de copilot-api** :
```bash
# V√©rifier si le patch existe toujours
grep -n "FIX #174" ~/.nvm/versions/node/v22.18.0/lib/node_modules/copilot-api/dist/main.js

# Si vide ‚Üí r√©-appliquer le patch
```

#### Suivi de l'issue officielle

Surveille [copilot-api#174](https://github.com/ericc-ch/copilot-api/issues/174) pour un fix officiel dans une future version.

Une fois le fix int√©gr√© officiellement :
```bash
npm update -g copilot-api  # Mettre √† jour
# Plus besoin du patch manuel
```

---

## ‚ö†Ô∏è MCP Schema Validation Error (GPT-4.1)

### Symptom

Quand tu utilises GPT-4.1, tu vois des erreurs API dans Claude Code :

```bash
COPILOT_MODEL=gpt-4.1 ccc
‚ùØ 1+1
API Error: 400 {"error":{"message":"Invalid schema for function 'mcp__grepai__grepai_index_status':
In context=(), object schema missing properties.","code":"invalid_function_parameters"}}
```

**Ces erreurs apparaissent √©galement dans les logs de copilot-api** (terminal o√π `copilot-api start` tourne) :

```
ERROR  HTTP error: { error:
   { message:
      "Invalid schema for function 'mcp__grepai__grepai_index_status': In context=(), object schema missing properties.",
     code: 'invalid_function_parameters' } }
```

### Cause

**GPT-4.1 applique une validation stricte des sch√©mas JSON** pour les outils MCP (Model Context Protocol). Certains serveurs MCP ont des sch√©mas incomplets ou invalides qui passent avec Claude (permissif) mais √©chouent avec GPT-4.1.

**Probl√®me typique** : Sch√©ma d√©clar√© comme `"type": "object"` sans d√©finir `"properties": {}`, ce qui est techniquement invalide selon JSON Schema.

**Serveurs MCP probl√©matiques connus**:
- ‚ùå `grepai`: `grepai_index_status` - object schema missing properties

### Solution

**Option 1: Utiliser Claude (100% compatible MCP)** ‚≠ê Recommand√©

```bash
ccc-sonnet   # Claude Sonnet 4.5 (d√©faut)
ccc-opus     # Claude Opus 4.5 (meilleure qualit√©)
ccc-haiku    # Claude Haiku 4.5 (ultra rapide)
```

**Avantages**:
- ‚úÖ 100% compatible avec tous les serveurs MCP
- ‚úÖ Validation permissive (accepte sch√©mas imparfaits)
- ‚úÖ Meilleure qualit√© que GPT-4.1

**Option 2: D√©sactiver le serveur MCP probl√©matique**

√âditez `~/.claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    // "grepai": {          <- Comment√© ou supprim√©
    //   "command": "grepai",
    //   "args": ["mcp-serve"]
    // },
    "playwright": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-playwright"]
    }
  }
}
```

Puis relancez Claude Code.

**Option 3: Reporter le probl√®me au mainteneur**

Si un serveur MCP que tu utilises a ce probl√®me, ouvre une issue:
- grepai: https://github.com/grepAI/grepai/issues
- Autres serveurs: Cherche le repo GitHub du serveur

### Diagnostic

**Quand tu vois l'erreur "Invalid schema for function 'mcp__...'", lance le diagnostic** :

```bash
# V√©rifier tous les serveurs MCP configur√©s
mcp-check.sh

# Avec analyse des logs r√©cents de Claude Code
mcp-check.sh --parse-logs
```

Le script identifiera le serveur MCP probl√©matique et te proposera 4 solutions.

**Output exemple**:
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
MCP Server Compatibility Check
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Found 3 MCP server(s) configured:

‚îÅ‚îÅ‚îÅ grepai ‚îÅ‚îÅ‚îÅ
Command: grepai mcp-serve
‚úì Command installed
‚úó Known compatibility issue:
  grepai_index_status: object schema missing properties
  Impact: Fails with GPT-4.1 (strict validation)

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Summary
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Servers checked: 3
Compatibility issues: 1

‚ïê‚ïê‚ïê Recommendations ‚ïê‚ïê‚ïê

Option 1: Use Claude models (100% MCP compatible)
  ccc-sonnet   # Claude Sonnet 4.5
  ccc-opus     # Claude Opus 4.5

Option 2: Disable problematic MCP servers
  Edit: /Users/you/.claude/claude_desktop_config.json
  Remove or comment out problematic servers

Option 3: Report issue to MCP server maintainer
  Example: https://github.com/grepAI/grepai/issues
```

### Comparaison: Claude vs GPT-4.1

| Aspect | Claude | GPT-4.1 |
|--------|--------|---------|
| **Validation MCP** | Permissive | Stricte |
| **Sch√©mas imparfaits** | ‚úÖ Accepte | ‚ùå Rejette |
| **Compatibilit√©** | 100% | ~80% (selon serveurs) |
| **Recommandation** | ‚≠ê D√©faut | Backup si Claude indisponible |

### Pourquoi Claude est meilleur pour copilot-api

1. **100% compatible endpoint `/chat/completions`**
2. **100% compatible MCP tools (validation permissive)**
3. **Pas de breaking changes sur updates MCP**
4. **Meilleure qualit√© de code g√©n√©r√©e**

**Conclusion**: Pour une exp√©rience sans friction avec copilot-api, **utilise Claude par d√©faut**.

---

## ü§ñ Gemini Agentic Mode Issues (copilot-api)

### Symptom

When using Gemini models with tool-calling (file creation, MCP tools, etc.), you experience:

```bash
COPILOT_MODEL=gemini-3-pro-preview ccc
‚ùØ Create a file called hello.txt with "test"

# Possible symptoms:
# 1. No response or timeout
# 2. Error: model_not_supported
# 3. Error: invalid_request_body
# 4. Error: INVALID_ARGUMENT
# 5. File not created despite "success" message
```

**Simple prompts work fine**:
```bash
COPILOT_MODEL=gemini-3-pro-preview ccc -p "1+1"
‚úÖ Returns: 2
```

**Agentic prompts fail**:
```bash
COPILOT_MODEL=gemini-3-pro-preview ccc -p "Create hello.txt"
‚ùå No file created, errors in logs
```

### Cause

**Root issue**: copilot-api translates Claude tool calling format ‚Üí OpenAI format ‚Üí Gemini format. This translation chain introduces incompatibilities:

1. **Tool Format Mismatch**: Claude uses Anthropic tool schema, Gemini expects Google-specific format
2. **Subagent Calls**: Claude Code spawns subagents (Task tool) that may not work correctly with Gemini
3. **Preview Model Instability**: `gemini-3-*-preview` models are experimental and may have incomplete tool support

**Issue reference**: [copilot-api#151](https://github.com/ericc-ch/copilot-api/issues/151)

### Diagnosis

Run automated diagnostic to identify the exact issue:

```bash
# In cc-copilot-bridge project
cd /path/to/cc-copilot-bridge

# Run test suite
./scripts/test-gemini.sh

# View results
cat debug-gemini/summary.txt
cat debug-gemini/diagnostic-report.md
```

**Diagnostic tests**:
| Test | Scenario | What It Checks |
|------|----------|----------------|
| 1 | Simple calculation | Baseline (non-agentic) |
| 2 | File creation | Direct tool calling |
| 3 | MCP grep tool | MCP schema compatibility |
| 4 | Subagent workaround | If routing through GPT fixes issue |
| 5 | Gemini 2.5 stable | Stable model comparison |

**Decision tree**:
```
Test 1 fails ‚Üí copilot-api auth/config issue
Test 2 fails, Test 1 OK ‚Üí Tool format incompatibility
Test 3 fails ‚Üí MCP schema validation issue (see "MCP Schema Validation Error" section)
Test 4 succeeds, Test 2 fails ‚Üí Confirms subagent routing fixes issue
Test 5 succeeds, Test 2 fails ‚Üí Gemini 3 preview limitation
```

### Solutions

**Option 1: Use Stable Gemini 2.5 Pro** ‚≠ê Recommended

```bash
COPILOT_MODEL=gemini-2.5-pro ccc
# OR
ccc-gemini  # Alias for gemini-2.5-pro

‚ùØ Create hello.txt with "test"
‚úÖ Works reliably for most scenarios
```

**Pros**:
- ‚úÖ More stable than preview models
- ‚úÖ Better tool calling support
- ‚úÖ Production-ready

**Cons**:
- ‚ö†Ô∏è May still have occasional issues with complex multi-tool workflows
- ‚ö†Ô∏è Deprecation scheduled: 17 Feb 2026 ‚Üí migrate to gemini-3-pro-preview once stable

**Option 2: Use Subagent Workaround (Gemini 3 Preview)**

For preview models, route complex operations through a stable subagent:

```bash
# Manual usage
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc

‚ùØ Create hello.txt with "test"
‚úÖ Subagent (GPT-5-mini) handles tool calls
```

**How it works**:
- Main agent: Gemini 3 (planning, reasoning)
- Subagent: GPT-5-mini (tool execution)
- When Claude Code spawns Task tool ‚Üí uses GPT instead of Gemini

**Pros**:
- ‚úÖ Keeps Gemini 3 for main reasoning
- ‚úÖ Stable tool execution via GPT
- ‚úÖ No need to disable MCP servers

**Cons**:
- ‚ö†Ô∏è Slight latency increase (2 models involved)
- ‚ö†Ô∏è Mixed model behavior

**Option 3: Use Claude Models (100% Compatible)** üöÄ Best Quality

```bash
ccc-sonnet  # Claude Sonnet 4.5 (default, balanced)
ccc-opus    # Claude Opus 4.5 (best quality)
ccc-haiku   # Claude Haiku 4.5 (fastest)

‚ùØ Create hello.txt with "test"
‚úÖ Works flawlessly, no workarounds needed
```

**Pros**:
- ‚úÖ 100% tool calling compatibility
- ‚úÖ Best agentic performance
- ‚úÖ No translation issues (native Anthropic format)
- ‚úÖ Best code quality

**Cons**:
- None (Claude via Copilot is the gold standard)

**Option 4: Use GPT Models (Reliable Alternative)**

```bash
COPILOT_MODEL=gpt-4.1 ccc    # Balanced, 0x premium
COPILOT_MODEL=gpt-5 ccc      # Advanced reasoning, 1x premium
COPILOT_MODEL=gpt-5-mini ccc # Ultra fast, 0x premium

‚ùØ Create hello.txt with "test"
‚úÖ Reliable tool calling (with MCP exclusions if needed)
```

**Pros**:
- ‚úÖ Stable tool calling
- ‚úÖ Good agentic performance
- ‚úÖ Fast responses

**Cons**:
- ‚ö†Ô∏è MCP schema validation (see "MCP Schema Validation Error" section)
- ‚ö†Ô∏è May need to disable `grepai` MCP server

### Automated Workaround (claude-switch Integration)

The subagent workaround can be automated in `claude-switch` script:

```bash
# In ~/bin/claude-switch, add to _run_copilot() function:

# Gemini workaround: auto-set subagent for preview models
if [[ "$COPILOT_MODEL" == gemini-3-*-preview ]]; then
    export CLAUDE_CODE_SUBAGENT_MODEL="${CLAUDE_CODE_SUBAGENT_MODEL:-gpt-5-mini}"
    _log "INFO" "Gemini preview detected: subagent=$CLAUDE_CODE_SUBAGENT_MODEL"
fi
```

**Benefits**:
- üîÑ Automatic workaround activation
- üìù Logged in session logs
- üéØ Only affects Gemini preview models

### Model Compatibility Matrix

| Model | Simple Prompts | Agentic/Tools | Status | Recommendation |
|-------|----------------|---------------|--------|----------------|
| `claude-sonnet-4.5` | ‚úÖ Excellent | ‚úÖ Excellent | Stable | ‚≠ê **Best choice** |
| `claude-opus-4.5` | ‚úÖ Excellent | ‚úÖ Excellent | Stable | ‚≠ê Best quality |
| `gpt-4.1` | ‚úÖ Excellent | ‚úÖ Good | Stable | ‚úÖ Reliable |
| `gpt-5` | ‚úÖ Excellent | ‚úÖ Good | Stable | ‚úÖ Advanced reasoning |
| `gemini-2.5-pro` | ‚úÖ Good | ‚ö†Ô∏è Fair | Deprecating 2/17/26 | ‚ö†Ô∏è Use with caution |
| `gemini-3-pro-preview` | ‚úÖ Good | ‚ùå Poor | Experimental | ‚ùå Use subagent workaround |
| `gemini-3-flash-preview` | ‚úÖ Good | ‚ùå Poor | Experimental | ‚ùå Use subagent workaround |

### Known Limitations

**Gemini 3 Preview Models**:
- ‚ùå Direct tool calling unreliable
- ‚ùå Subagent spawning may fail
- ‚ùå MCP tool execution inconsistent
- ‚ö†Ô∏è File operations may silently fail

**Gemini 2.5 Pro** (Stable but deprecating):
- ‚ö†Ô∏è Occasional tool calling failures
- ‚ö†Ô∏è Complex multi-tool workflows problematic
- ‚ö†Ô∏è Deprecation: 17 Feb 2026

**Recommended Migration Path**:
```
Current: gemini-2.5-pro
‚Üì
Short-term: gemini-2.5-pro + monitor stability
‚Üì
If issues: Switch to claude-sonnet-4.5 (ccc-sonnet)
‚Üì
When stable: Migrate to gemini-3-pro-preview (with subagent)
```

### Manual Testing

If you prefer manual testing:

```bash
# Create test directory
cd /tmp && mkdir -p gemini-test && cd gemini-test

# Test 1: Baseline (should work)
COPILOT_MODEL=gemini-3-pro-preview ccc -p "Calculate 1+1"

# Test 2: File creation (may fail)
COPILOT_MODEL=gemini-3-pro-preview ccc -p "Create hello.txt with 'test'"

# Test 3: With subagent workaround (should work)
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini \
  ccc -p "Create hello2.txt with 'test'"

# Test 4: Stable model (should work)
COPILOT_MODEL=gemini-2.5-pro ccc -p "Create hello3.txt with 'test'"

# Verify files created
ls -la hello*.txt
```

### Verify copilot-api Logs

If you see errors, check copilot-api logs:

```bash
# Terminal 1: Start copilot-api in verbose mode
pkill -f copilot-api || true
copilot-api start -v 2>&1 | tee copilot-api-verbose.log

# Terminal 2: Run tests
# ... execute tests ...

# Terminal 1: Look for errors
grep -iE "(error|invalid|model_not_supported)" copilot-api-verbose.log
```

**Common error patterns**:
```
ERROR  HTTP error: { error: { message: 'model_not_supported' } }
ERROR  Invalid schema for function 'mcp__...'
ERROR  INVALID_ARGUMENT: tool_config.function_calling_config ...
```

### Troubleshooting Steps

1. **Verify copilot-api is running**:
   ```bash
   nc -z localhost 4141 && echo "‚úÖ Running" || echo "‚ùå Not running"
   ```

2. **Check model availability**:
   ```bash
   # In copilot-api logs, you should see:
   # Available models: claude-*, gpt-*, gemini-*
   ```

3. **Test with working model first**:
   ```bash
   # Establish baseline with Claude
   ccc-sonnet -p "1+1"
   # If this fails ‚Üí copilot-api issue, not Gemini-specific
   ```

4. **Run automated diagnostic**:
   ```bash
   ./scripts/test-gemini.sh
   cat debug-gemini/diagnostic-report.md
   ```

5. **Analyze logs**:
   ```bash
   ./scripts/analyze-copilot-logs.sh debug-gemini/copilot-api-verbose.log
   ```

### Best Practices

**For Production Code**:
```bash
ccc-sonnet   # 100% reliable, best quality
```

**For Experimentation with Gemini**:
```bash
# Use subagent workaround
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc
```

**For Quick Tasks**:
```bash
ccc-haiku    # Fast, reliable, no Gemini complexity
```

### References

- [copilot-api Issue #151](https://github.com/ericc-ch/copilot-api/issues/151) - Gemini model compatibility
- [Gemini API Tool Calling Docs](https://ai.google.dev/gemini-api/docs/function-calling)
- `scripts/test-gemini.sh` - Automated diagnostic suite
- `debug-gemini/README.md` - Testing workspace documentation

---

## üîå Provider Not Running

### Copilot: Port 4141 Not Responding

**Symptom**:
```bash
ccc
ERROR: copilot-api not running on :4141
  Start it with: copilot-api start
```

**Solution**:
```bash
# Start copilot-api
copilot-api start

# If OAuth expired, re-authenticate
copilot-api stop
copilot-api start
```

### Ollama: Port 11434 Not Responding

**Symptom**:
```bash
cco
ERROR: Ollama not running on :11434
  Start it with: ollama serve
```

**Solution**:
```bash
# Check Homebrew service
brew services info ollama

# Restart if needed
brew services restart ollama

# Verify it's running
curl http://localhost:11434/api/tags
```

---

## üíæ Out of Memory / Slow Performance

### Symptom

```bash
ollama ps
NAME                          SIZE      PROCESSOR
devstral-small-2              12 GB     50% GPU  # Should be ~15 GB, 100% GPU
```

### Cause

Not enough RAM or model not fully loaded.

### Solution

1. **Check available RAM**:
   ```bash
   # macOS
   vm_stat | grep free

   # Should have 20GB+ free for devstral-small-2
   ```

2. **Use Granite4 if RAM-limited** (70% less VRAM with hybrid Mamba architecture):
   ```bash
   ollama pull ibm/granite4:small-h
   OLLAMA_MODEL=ibm/granite4:small-h cco
   ```

3. **Close other applications** to free RAM.

---

## üö´ Permission Denied Errors

### Symptom

```bash
claude-switch: permission denied
```

### Solution

```bash
chmod +x ~/bin/claude-switch
chmod +x ~/bin/ollama-check.sh
chmod +x ~/bin/ollama-optimize.sh
```

---

## üîÑ Changes Not Applied After Optimization

### Symptom

After running `ollama-optimize.sh`, performance unchanged.

### Cause

Environment variables set but service not restarted.

### Solution

```bash
# Restart Ollama service
brew services restart ollama

# Verify variables are set
launchctl getenv OLLAMA_FLASH_ATTENTION  # Should show: 1
launchctl getenv OLLAMA_CONTEXT_LENGTH   # Should show: 8192

# Wait 10 seconds for service to start
sleep 10

# Test
ollama ps
```

---

## üìä Model Shows Wrong Identity

### Symptom

```bash
cco
‚ùØ who are you?
‚è∫ I am Claude, created by Anthropic...
```

But you're using Devstral (not Claude).

### Explanation

**Not a bug**. The local model sees Claude Code's system prompt and adopts that identity. This is normal behavior for instruction-tuned models.

**It doesn't affect**:
- Code quality
- Performance
- Functionality

The model is still **Devstral** (or Granite4), just confused about its identity from the prompt.

---

## üîç How to Verify Which Provider is Active

### Method 1: Check Logs

```bash
tail -5 ~/.claude/claude-switch.log
```

**Look for**:
```
[INFO] Provider: Ollama Local - Model: devstral-64k
[INFO] Session started: mode=ollama:...
```

### Method 2: Check Process Variables

```bash
# Find Claude Code PID
ps aux | grep "claude --model"

# Check environment (replace PID)
cat /proc/PID/environ | tr '\0' '\n' | grep ANTHROPIC
```

**Expected for Ollama**:
```
ANTHROPIC_BASE_URL=http://localhost:11434
ANTHROPIC_AUTH_TOKEN=ollama
```

### Method 3: Monitor Network Traffic

```bash
# Terminal 1: Monitor Ollama port
sudo tcpdump -i lo0 -A 'tcp port 11434'

# Terminal 2: Launch cco and send prompt
# You should see JSON traffic in Terminal 1
```

---

## üÜò Get Help

If issues persist:

1. **Run full diagnostic**:
   ```bash
   ollama-check.sh > diagnostic.txt
   ```

2. **Check provider status**:
   ```bash
   ccs  # Status of all providers
   ```

3. **Review logs**:
   ```bash
   tail -50 ~/.claude/claude-switch.log
   ```

4. **Report issue** with:
   - Output of `ollama-check.sh`
   - Last 20 lines of `~/.claude/claude-switch.log`
   - Output of `ollama ps`
   - Your project size (file count)

---

## üìö Related Documentation

- [README.md](README.md) - Full documentation
- [OPTIMISATION-M4-PRO.md](OPTIMISATION-M4-PRO.md) - Performance optimization guide
- [STATUS.md](STATUS.md) - Implementation status
- [COMMANDS.md](COMMANDS.md) - Command reference

---

**Last Updated**: 2026-01-21
**Version**: 1.1.0

---

## üìö Related Documentation

- [FAQ](FAQ.md) - Frequently asked questions
- [Decision Trees](DECISION-TREES.md) - Choose the right command/model
- [Best Practices](BEST-PRACTICES.md) - Strategic usage patterns
- [Security Guide](SECURITY.md) - Privacy and data flow
- [Quick Start Guide](../QUICKSTART.md) - Installation guide

---

**Back to**: [Documentation Index](README.md) | [Main README](../README.md)
