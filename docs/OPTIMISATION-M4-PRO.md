# Apple Silicon Optimization Guide (M1/M2/M3/M4)

**Reading time**: 15 minutes | **Skill level**: Advanced | **Ollama version**: 0.14.2+ | **Last updated**: 2026-01-22

**Target**: Qwen2.5-Coder-32B Q4_K_M on M4 Pro 48GB

---

## Configuration Appliqu√©e

### Variables d'Environnement

```bash
OLLAMA_FLASH_ATTENTION=1          # Flash Attention pour Gemma 3/Qwen 3
OLLAMA_KV_CACHE_TYPE=q4_0         # KV Cache quantization - reduces memory by ~75%
OLLAMA_NUM_PARALLEL=4             # 4 requ√™tes parall√®les
OLLAMA_MAX_LOADED_MODELS=3        # 3 mod√®les charg√©s simultan√©ment
OLLAMA_CONTEXT_LENGTH=8192        # Contexte optimal pour 32B
OLLAMA_KEEP_ALIVE=24h             # Garde le mod√®le en RAM 24h
```

> **New in 2025**: `OLLAMA_KV_CACHE_TYPE=q4_0` enables KV cache quantization, reducing cache memory from ~48GB to ~12GB for 64K context. This makes 64K context feasible on 32GB machines.

### Performances Attendues

| M√©trique | Valeur |
|----------|--------|
| RAM | 20-24 GB |
| Vitesse (4K context) | 26-39 tokens/sec |
| Vitesse (8K context) | 24-32 tokens/sec |
| Qualit√© | 100% (SOTA) |

---

## ‚ö†Ô∏è OLLAMA_CONTEXT_LENGTH: Le Param√®tre Critique

### Comprendre le Contexte

**`OLLAMA_CONTEXT_LENGTH`** d√©termine combien de tokens le mod√®le peut traiter en une seule fois. Ce param√®tre a un **impact majeur** sur les performances avec Claude Code.

### Pourquoi 8192 par D√©faut ?

La valeur `8192` (8K tokens) est un **compromis optimal** pour:
- ‚úÖ Vitesse maximale (26-39 tok/s)
- ‚úÖ RAM raisonnable (26 GB sur GPU)
- ‚úÖ Qualit√© constante

**Mais attention** : Cette valeur convient uniquement aux **petits projets** avec Claude Code.

### Le Probl√®me avec Claude Code

Claude Code envoie un **contexte initial volumineux** comprenant :

```
Contexte typique (projet moyen-grand) : ~60,000 tokens
‚îú‚îÄ Indexation projet (Memory files) : 22,200 tokens
‚îú‚îÄ Outils syst√®me                   : 19,800 tokens
‚îú‚îÄ Serveurs MCP                     : 8,400 tokens
‚îú‚îÄ Agents custom                    : 3,400 tokens
‚îî‚îÄ Prompt syst√®me                   : 3,800 tokens
```

**Si `OLLAMA_CONTEXT_LENGTH=8192`** :
- ‚ö†Ô∏è **Truncation massive** : 87% du contexte perdu
- üêå **Lenteur extr√™me** : 2-6 minutes par r√©ponse
- üíª **CPU surcharg√©** : Retraitement constant
- üî• **Ventilateurs** : Mac qui souffle

### Trade-offs par Configuration

| Contexte | RAM | Vitesse | Cas d'Usage | Projet |
|----------|-----|---------|-------------|---------|
| **4096** | 24 GB | ‚ö°‚ö°‚ö° 35-45 tok/s | Scripts simples | <50 fichiers |
| **8192** | 26 GB | ‚ö°‚ö° 26-39 tok/s | **Petits projets** | <500 fichiers |
| **16384** | 30-32 GB | üê¢ 15-25 tok/s | Projets moyens | 500-2K fichiers |
| **32768** | 36-40 GB | üêå 8-15 tok/s | Grands projets | >2K fichiers |

### Recommandations par Type de Projet

#### Petits Projets (<500 fichiers)
**Config actuelle (8K) est parfaite** ‚úÖ

```bash
# Rien √† changer
OLLAMA_CONTEXT_LENGTH=8192
```

**Exemples** :
- Scripts utilitaires
- CLI tools
- Petites applications
- Projets d'apprentissage

#### Projets Moyens (500-2K fichiers)
**Augmenter √† 16K** ‚ö†Ô∏è

```bash
launchctl setenv OLLAMA_CONTEXT_LENGTH 16384
brew services restart ollama
```

**Trade-off** :
- ‚úÖ Claude Code fonctionne correctement
- ‚ùå Vitesse r√©duite (15-25 tok/s au lieu de 26-39)
- ‚ùå +4-6 GB RAM

**Exemples** :
- Applications web standard
- APIs REST moyennes
- Projets Next.js/React

#### Grands Projets (>2K fichiers)
**Augmenter √† 32K** üêå **OU utiliser Copilot/Anthropic** ‚ö°

```bash
# Option 1: Ollama 32K (lent mais priv√©)
launchctl setenv OLLAMA_CONTEXT_LENGTH 32768
brew services restart ollama

# Option 2: Copilot (rapide et gratuit)
ccc  # 1-3 secondes par r√©ponse

# Option 3: Anthropic (rapide et payant)
ccd  # 1-2 secondes par r√©ponse
```

**Exemples** :
- Monorepos
- Applications enterprise
- Projets avec multiples MCP servers
- Codebases complexes

### Tests Comparatifs (M4 Pro 48GB, Qwen2.5-Coder-32B)

#### Test 1: Petit Projet (8K Context)
```bash
cd ~/simple-script/
cco
‚ùØ write a fibonacci function
‚è±Ô∏è R√©ponse: 3-5 secondes ‚úÖ
üìä Qualit√©: Excellente
```

#### Test 2: Grand Projet (8K Context) ‚ùå
```bash
cd ~/monorepo-app/  # ~60K tokens de contexte
cco
‚ùØ 1+1 ?
‚è±Ô∏è R√©ponse: 2-6 MINUTES ‚ùå
üìä Qualit√©: Incoh√©rente (contexte tronqu√©)
üíª RAM: 26 GB (mais CPU √† 100%)
```

#### Test 3: Grand Projet (32K Context) üêå
```bash
cd ~/monorepo-app/
# Apr√®s avoir augment√© le contexte √† 32K
cco
‚ùØ 1+1 ?
‚è±Ô∏è R√©ponse: 30-60 secondes ‚ö†Ô∏è
üìä Qualit√©: Excellente
üíª RAM: 38 GB
```

#### Test 4: Grand Projet avec Copilot ‚ö°
```bash
cd ~/monorepo-app/
ccc
‚ùØ 1+1 ?
‚è±Ô∏è R√©ponse: 1-2 secondes ‚úÖ
üìä Qualit√©: Excellente
üíª RAM: N√©gligeable (API distante)
```

### Comment V√©rifier Votre Contexte

**Pendant une session Claude Code**, tapez `/context` pour voir :
- Nombre de tokens utilis√©s
- R√©partition par cat√©gorie
- Espace libre restant

Si **"Free space" est n√©gatif** ou tr√®s faible ‚Üí votre projet d√©passe la capacit√© configur√©e.

### Strat√©gie Recommand√©e

Pour optimiser votre workflow :

```bash
# 1. Petits projets/scripts : Ollama 8K (rapide)
cd ~/scripts/
cco  # 3-10 secondes par r√©ponse

# 2. Projets moyens : Copilot (rapide + gratuit)
cd ~/web-app/
ccc  # 1-3 secondes par r√©ponse

# 3. Gros projets : Copilot ou Anthropic
cd ~/monorepo/
ccc  # ou ccd
```

**Ollama 32K uniquement si** :
- ‚úÖ Code ultra-confidentiel (100% local requis)
- ‚úÖ Pas de connexion internet
- ‚úÖ Temps de r√©ponse de 30-60s acceptable

### Conclusion

Le param√®tre `OLLAMA_CONTEXT_LENGTH=8192` est **optimal pour la vitesse**, mais **inadapt√© aux grands projets** avec Claude Code.

**Choix √† faire** :
1. **Petits projets** : Garder 8K (rapide) ‚úÖ
2. **Projets moyens** : 16K (compromis) ‚ö†Ô∏è
3. **Grands projets** : Copilot/Anthropic (rapide) ou 32K (lent mais priv√©) üêå

---

## Scripts Cr√©√©s

### 1. `~/bin/ollama-optimize.sh`

Script d'optimisation automatique qui :
- ‚úÖ Configure les variables d'environnement avec `launchctl`
- ‚úÖ Red√©marre le service Ollama
- ‚úÖ V√©rifie la disponibilit√© de l'API

**Utilisation** :
```bash
ollama-optimize.sh
```

### 2. `~/bin/ollama-check.sh`

Script de diagnostic complet (11 v√©rifications) :
1. Version Ollama
2. Processus en cours
3. Service Homebrew
4. Mod√®les install√©s
5. Connectivit√© API
6. Ports r√©seau
7. Variables d'environnement
8. Espace disque
9. Test du mod√®le
10. M√©moire syst√®me
11. Int√©gration claude-switch

**Utilisation** :
```bash
ollama-check.sh
```

---

## Test de l'Int√©gration Compl√®te

### Test 1 : Via claude-switch

```bash
# Lancer Claude Code avec Ollama
cco
```

**Prompt de test recommand√©** :
```
Write a React component for user authentication with the following features:
- Email/password form with validation
- JWT token handling
- Error state management
- Loading indicator
- TypeScript types
```

**Ce que vous devriez observer** :
- ‚úÖ Mod√®le se charge en RAM (~20-24GB)
- ‚úÖ G√©n√©ration rapide (26-39 tok/s)
- ‚úÖ Code de qualit√© SOTA avec :
  - Gestion d'erreurs compl√®te
  - Validation robuste
  - Types TypeScript pr√©cis
  - Bonnes pratiques React
  - Documentation claire

### Test 2 : Monitoring RAM

**Terminal 1** : Lancer Claude Code
```bash
cco
```

**Terminal 2** : Monitor RAM en temps r√©el
```bash
watch -n 2 'ps aux | grep ollama | grep -v grep | awk "{printf \"Ollama: %.2f GB\\n\", \$6/1024/1024}"'
```

**R√©sultat attendu** :
```
Ollama: 0.03 GB  # Avant le premier prompt
‚Üì
Ollama: 20.45 GB # Apr√®s chargement du mod√®le (10-20 sec)
```

### Test 3 : Benchmark Performance

```bash
time ollama run qwen2.5-coder:32b-instruct "Write a binary search implementation in Python with edge case handling"
```

**M√©trique attendue** :
- Temps de r√©ponse : 5-8 secondes pour ~200 tokens
- Vitesse effective : 25-40 tok/s

---

## Comparaison des Providers

Test effectu√© avec le prompt : "Refactor this function to be more maintainable"

| Provider | Mod√®le | Vitesse | Qualit√© | Co√ªt | Privacy |
|----------|--------|---------|---------|------|---------|
| **Ollama** | Qwen2.5-32B | 26-39 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Gratuit | 100% local |
| **Copilot** | Claude Opus 4.5 | ~60 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Inclus (abo) | Cloud |
| **Anthropic** | Claude Sonnet 4.5 | ~80 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $15-20/mois | Cloud |

**Recommandations d'usage** :

| Sc√©nario | Provider Recommand√© |
|----------|---------------------|
| Code propri√©taire/sensible | `cco` (Ollama) |
| It√©ration rapide/prototypage | `ccc` (Copilot) |
| Review critique/production | `ccd` (Anthropic) ou `ccc-opus` |
| Exploration/questions rapides | `ccc-haiku` (rapide, gratuit) |

---

## V√©rification Post-Installation

### Checklist Compl√®te

- [x] Ollama 0.14.2 install√©
- [x] Mod√®le Qwen2.5-Coder-32B t√©l√©charg√© (19GB)
- [x] Variables d'optimisation configur√©es
- [x] Service Ollama red√©marr√©
- [x] API responsive sur :11434
- [x] claude-switch install√© et test√©
- [x] Alias shell configur√©s (ccd, ccc, cco, ccs)
- [x] Scripts de diagnostic et optimisation cr√©√©s

### Statut Actuel

```bash
ollama-check.sh
```

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  R√âSUM√â
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

   Ollama actif:      ‚úÖ
   API accessible:    ‚úÖ
   Mod√®le 32B:        ‚úÖ
   claude-switch:     ‚úÖ
```

---

## Commandes R√©capitulatives

| Commande | Description |
|----------|-------------|
| `ccd` | Anthropic Direct (meilleure qualit√©) |
| `ccc` | GitHub Copilot (gratuit avec abo) |
| `cco` | Ollama Local (privacy, offline) |
| `ccs` | Status de tous les providers |
| `ccc-opus` | Copilot avec Claude Opus 4.5 |
| `ccc-sonnet` | Copilot avec Claude Sonnet 4.5 |
| `ccc-haiku` | Copilot avec Claude Haiku 4.5 (rapide) |
| `ccc-gpt` | Copilot avec GPT-5.2 Codex |
| `ollama-check.sh` | Diagnostic complet Ollama |
| `ollama-optimize.sh` | R√©appliquer optimisations |

---

## Troubleshooting

### Probl√®me : Mod√®le ne se charge pas en RAM

**Sympt√¥me** : RAM reste √† ~0.03GB apr√®s lancement

**Solution** :
```bash
# Forcer le rechargement
ollama rm qwen2.5-coder:32b-instruct
ollama pull qwen2.5-coder:32b-instruct-q4_k_m

# Tester directement
ollama run qwen2.5-coder:32b-instruct "Write a hello world"
```

### Probl√®me : Performance inf√©rieure √† 26 tok/s

**V√©rifications** :
1. Confirmer version 0.14.2 : `ollama --version`
2. V√©rifier RAM disponible : `top -o MEM` (devrait avoir >30GB libre)
3. R√©appliquer optimisations : `ollama-optimize.sh`
4. Red√©marrer : `brew services restart ollama`

### Probl√®me : API ne r√©pond pas

**Solution** :
```bash
# V√©rifier processus
ps aux | grep ollama

# Red√©marrer service
brew services restart ollama

# V√©rifier port
lsof -i :11434
```

---

## Sources

- **Perplexity Research** : Optimisation Ollama 0.14.2 pour Apple Silicon (2026-01-21)
- **Benchmark officiel** : Qwen2.5-Coder-32B sur M4 Pro (26.85 tok/s @ Q4_K_M)
- **Documentation Ollama** : Variables d'environnement et optimisations

---

## Next Steps

1. ‚úÖ **Tester l'int√©gration** : `cco` et v√©rifier le chargement du mod√®le
2. ‚úÖ **Monitorer la RAM** : Confirmer 20-24GB apr√®s premier prompt
3. ‚úÖ **Benchmark performance** : Mesurer tok/s effectif
4. ‚è≥ **Workflows quotidiens** : Utiliser selon les recommandations ci-dessus

---

**Installation compl√®te et optimis√©e !** üéâ

Le syst√®me multi-provider est maintenant pr√™t avec :
- 3 providers (Anthropic, Copilot, Ollama)
- 25+ mod√®les disponibles via Copilot
- Ollama optimis√© pour M4 Pro 48GB
- Documentation compl√®te (10 fichiers, ~52KB)
- Scripts de diagnostic et optimisation

---

## üìö Related Documentation

- [Best Practices](BEST-PRACTICES.md) - Performance optimization strategies
- [Architecture](ARCHITECTURE.md) - Ollama integration details
- [Troubleshooting](TROUBLESHOOTING.md) - Ollama slow performance issues
- [FAQ](FAQ.md) - Ollama performance questions

---

**Back to**: [Documentation Index](README.md) | [Main README](../README.md)
